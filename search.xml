<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[罗翔讲刑法--金句笔记]]></title>
    <url>%2F2020%2F03%2F21%2F%E7%BD%97%E7%BF%94%E9%87%91%E5%8F%A5%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[我们之所以做一个道德的人，不是为了追求幸福，而只是因为这本身就无愧于我们已经拥有的幸福。 一个人不能处分自己最重要的利益，因为自由不能以彻底放弃自由为代价。（这样的自由一定会导致强者对弱者的剥削。） 法益作为入罪的基础，而伦理作为出罪的依据。 犯罪是可怕的，但比犯罪更可怕的是不受约束的刑罚权力。 一次不公正的审判，其恶果甚至超过十次犯罪。因为犯罪虽是无视法律——好比污染了水流，而不公正的审判则毁坏法律——好比污染水源。（培根） 自从有刑法存在国家代替受害人私刑报复时开始，国家就承担了双重责任，因此表现出它的悖论性。刑法不仅要面对犯罪人保护国家，也要面对国家保护犯罪人，不单面对犯罪人也要面对检察官保护市民，成为公民反对私法专横和错误的大宪章。（德国法学家拉德布鲁赫） 如果刑罚权不受限制，那么一切正义都有可能被架空，而且往往是以正义的名义来架空正义。 权力导致腐败，绝对权力导致绝对腐败。 刑法是一根带哨子的皮鞭。 当立法权和司法权合二为一，独裁就不可避免，自由就荡然无存。（孟德斯鸠） 法治的精神在于限权。 如果衣服上出现了褶皱，司法机关可以用熨斗把它熨平，但如果衣服上出现了一个大洞，那就必须取决于立法机关把它织补。（英国丹宁勋爵） 法律是一种平衡的艺术，要在诸多对立价值中寻找一个平衡点。 有一种鸟是永远也关不住的，因为它的每片羽翼上都沾满了自由的光辉。（《肖申克的救赎》经典台词） 法律要倾听民众的声音，但是要超越民众的偏见。]]></content>
      <categories>
        <category>Law</category>
      </categories>
      <tags>
        <tag>law</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李弘毅ML课程作业-02：线性二元分类器]]></title>
    <url>%2F2020%2F03%2F21%2FMLHW2%2F</url>
    <content type="text"><![CDATA[作业描述​ 根据人们的个人资料，判断其年收入是否高于$50, 000。这其实是个二元分类问题，本文将以 logistic regression 和 generative model 两种方法达成分类目的。 ​ 作业提供了五个文件，其中在训练过程中，只有X_train、Y_train 和 X_test 这三个经过处理的档案会被使用到，train.csv 和 test.csv 是原始资料，供以参考。 Logistic Regression数据准备​ 先打开看一下数据集长什么样子。训练集数据的各种属性资料都已经数字化了；训练集的标签就是二元化的标签，年收入高于$50, 000为1，否则为0。 ​ 读入数据。由于第一行是数据说明，不必存到数组中，所以用next()函数跳过。数据集的第一列是id号，也不必存到数组中，所以数组存入的数据是从第二行第二列开始直到最后。 123456789101112131415161718import numpy as npnp.random.seed(0)X_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './output_&#123;&#125;.csv'# Parse csv files to numpy arraywith open(X_train_fpath) as f: next(f) X_train = np.array([line.strip('\n').split(',')[1:] for line in f], dtype=float)with open(Y_train_fpath) as f: next(f) Y_train = np.array([line.strip('\n').split(',')[1] for line in f], dtype=float)with open(X_test_fpath) as f: next(f) X_test = np.array([line.strip('\n').split(',')[1:] for line in f], dtype=float) ​ 先定义几个辅助函数。_normalize 函数用来对于数据集的特定列（某属性特征）进行归一化，注释有详细说明。_train_dev_split 函数用于将训练数据集划分为training set和 development set。我们拿不到 testing set 的正确标签，要用development set 来评估性能，而且也能防止过拟合。 12345678910111213141516171819202122232425262728293031def _normalize(X, train=True, specified_column = None, X_mean = None, X_std = None): # This function normalizes specific columns of X. # The mean and standard variance of training data will be reused when processing testing data. # # Arguments: # X: data to be processed # train: 'True' when processing training data, 'False' for testing data # specific_column: indexes of the columns that will be normalized. If 'None', all columns # will be normalized. # X_mean: mean value of training data, used when train = 'False' # X_std: standard deviation of training data, used when train = 'False' # Outputs: # X: normalized data # X_mean: computed mean value of training data # X_std: computed standard deviation of training data if specified_column == None: specified_column = np.arange(X.shape[1]) if train: X_mean = np.mean(X[:, specified_column], 0).reshape(1, -1) X_std = np.std(X[:, specified_column], 0).reshape(1, -1) X[:, specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8) return X, X_mean, X_stddef _train_dev_split(X, Y, dev_ratio=0.25): """This function splits data into training set and development set.""" train_size = int(len(X)*(1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:] ​ 进行归一化和划分训练集。training set 大小：48, 830 × 510；development set 大小：5, 426 × 510；testing set 大小：27, 622 × 510。 123456789101112# Normalize training and testing dataX_train, X_mean, X_std = _normalize(X_train, train=True)X_test, _, _ = _normalize(X_test, train=False, specified_column=None, X_mean=X_mean, X_std=X_std)# Split data into training set and development setdev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio)train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1] 一些辅助函数​ _shuffle 方法将X和Y的所有元素同时随机排序，X中某元素依然对应着在Y中对应着原先所对应的元素（data-label pair 不变）。_f 就是 logistic 回归函数，函数输出值在0到1之间，越接近于1，表示年收入高于$50, 000的概率越大。 \sigma (z) = 1/(1+e^{-z}) f_{w,b}=\sigma （\sum_{i}w_{i}x_{i}+b）12345678910111213141516171819202122232425262728293031def _shuffle(X, Y): # This function shuffles two equal-length list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return (X[randomize], Y[randomize])def _sigmoid(z): # Sigmoid function can be used to calculate probability. # To avoid overflow, minimum/maximum output value is set. return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))def _f(X, w, b): # This is the logistic regression function, parameterized by w and b # # Arguements: # X: input data, shape = [batch_size, data_dimension] # w: weight vector, shape = [data_dimension, ] # b: bias, scalar # Output: # predicted probability of each row of X being positively labeled, shape = [batch_size, ] return _sigmoid(np.matmul(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X # by rounding the result of logistic regression function. return np.round(_f(X, w, b)).astype(np.int) def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc 训练​ 使用的是交叉熵损失函数： L(f)=\sum_{n}C(f(x^{n}),\widehat{y}^{n}) C(f(x^{n}),\widehat{y}^{n})=-[\widehat{y}^{n}lnf(x^{n})+(1-\widehat{y}^{n})ln(1-f(x^{n}))]利用损失函数对权重的梯度值，权重更新公式如下。 w_{i} \leftarrow w_{i}-\eta \sum_{i}[-(\widehat{y}^{n}-f_{w,b}(x^{n}))x_{i}^{n}]交叉熵和梯度值计算代码： 123456789101112131415161718def _cross_entropy_loss(y_pred, Y_label): # This function computes the cross entropy. # # Arguements: # y_pred: probabilistic predictions, float vector # Y_label: ground truth labels, bool vector # Output: # cross entropy, scalar cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred)) return cross_entropydef _gradient(X, Y_label, w, b): # This function computes the gradient of cross entropy loss with respect to weight w and bias b. y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = -np.sum(pred_error * X.T, 1) b_grad = -np.sum(pred_error) return w_grad, b_grad ​ 使用mini-batch gradient descent 来训练。“訓練資料被分為許多小批次，針對每一個小批次，我們分別計算其梯度以及損失，並根據該批次來更新模型的參數。當一次迴圈完成，也就是整個訓練集的所有小批次都被使用過一次以後，我們將所有訓練資料打散並且重新分成新的小批次，進行下一個迴圈，直到事先設定的迴圈數量達成為止。”（来自作业demo） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Zero initialization for weights ans biasw = np.zeros((data_dim,)) b = np.zeros((1,))# Some parameters for training max_iter = 10batch_size = 8learning_rate = 0.2# Keep the loss and accuracy at every iteration for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Calcuate the number of parameter updatesstep = 1# Iterative trainingfor epoch in range(max_iter): # Random shuffle at the begging of each epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for idx in range(int(np.floor(train_size / batch_size))): X = X_train[idx*batch_size:(idx+1)*batch_size] Y = Y_train[idx*batch_size:(idx+1)*batch_size] # Compute the gradient w_grad, b_grad = _gradient(X, Y, w, b) # gradient descent update # learning rate decay with time w = w - learning_rate/np.sqrt(step) * w_grad b = b - learning_rate/np.sqrt(step) * b_grad step = step + 1 # Compute loss and accuracy of training set and development set y_train_pred = _f(X_train, w, b) Y_train_pred = np.round(y_train_pred) train_acc.append(_accuracy(Y_train_pred, Y_train)) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) / train_size) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.round(y_dev_pred) dev_acc.append(_accuracy(Y_dev_pred, Y_dev)) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev) / dev_size)print('Training loss: &#123;&#125;'.format(train_loss[-1]))print('Development loss: &#123;&#125;'.format(dev_loss[-1]))print('Training accuracy: &#123;&#125;'.format(train_acc[-1]))print('Development accuracy: &#123;&#125;'.format(dev_acc[-1])) 绘制 loss 和 accuracy 曲线1234567891011121314151617import matplotlib.pyplot as plt# Loss curveplt.plot(train_loss)plt.plot(dev_loss)plt.title('Loss')plt.legend(['train', 'dev'])plt.savefig('loss.png')plt.show()# Accuracy curveplt.plot(train_acc)plt.plot(dev_acc)plt.title('Accuracy')plt.legend(['train', 'dev'])plt.savefig('acc.png')plt.show() 保存测试集结果123456# Predict testing labelspredictions = _predict(X_test, w, b)with open(output_fpath.format('logistic'), 'w') as f: f.write('id,label\n') for i, label in enumerate(predictions): f.write('&#123;&#125;,&#123;&#125;\n'.format(i, label)) Probabilistic Generative Model数据准备​ 训练集和测试集的读入和归一化处理方法与logistic regression 一模一样。但是generative model 有可解析的最佳解，因此不必使用development set。 计算均值和协方差​ 在generative model 中，需要计算出最有可能产生这个训练集的数据分布的参数，也就是均值和协方差。确定了这两个参数，分布就确定了，就可以计算出测试集中某人属于年收入高于$50, 000那一类的概率。两个类别使用同样的协方差。 123456789101112131415161718# Compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])mean_0 = np.mean(X_train_0, axis = 0)mean_1 = np.mean(X_train_1, axis = 0) # Compute in-class covariancecov_0 = np.zeros((data_dim, data_dim))cov_1 = np.zeros((data_dim, data_dim))for x in X_train_0: cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# Shared covariance is taken as a weighted average of individual in-class covariance.cov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train_0.shape[0] + X_train_1.shape[0]) 计算权重和偏差​ 权重矩阵和偏差向量可以直接计算出来。这里求逆矩阵用到了SVD分解。 \boldsymbol{w}=\Sigma^{-1}(\mu_{1}-\mu_{2}) b=-\frac{1}{2}\mu_{1}^{T}\Sigma^{-1}\mu_{1}+\frac{1}{2}\mu_{2}^{T}\Sigma^{-1}\mu_{2}+ln\frac{N_{1}}{N_{2}}其中$\Sigma$表示协方差，$\mu$表示均值，$N$表示样本数。 1234567891011121314# Compute inverse of covariance matrix.# Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.u, s, v = np.linalg.svd(cov, full_matrices=False)inv = np.matmul(v.T * 1 / s, u.T)# Directly compute weights and biasw = np.dot(inv, mean_0 - mean_1)b = (-0.5) * np.dot(mean_0, np.dot(inv, mean_0)) + 0.5 * np.dot(mean_1, np.dot(inv, mean_1))\ + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0]) # Compute accuracy on training setY_train_pred = 1 - _predict(X_train, w, b)print('Training accuracy: &#123;&#125;'.format(_accuracy(Y_train_pred, Y_train))) 保存测试集结果123456# Predict testing labelspredictions = 1 - _predict(X_test, w, b)with open(output_fpath.format('generative'), 'w') as f: f.write('id,label\n') for i, label in enumerate(predictions): f.write('&#123;&#125;,&#123;&#125;\n'.format(i, label)) Report 請比較實作的 generative model 及 logistic regression 的準確率，何者較佳？請解釋為何有這種情況？ Generative Model Logistic Regression train 87.412% 88.569% 請實作 logistic regression 的正規化 (regularization)，並討論其對於你的模型準確率的影響。接著嘗試對正規項使用不同的權重 (lambda)，並討論其影響。 請說明你實作的 best model，其訓練方式和準確率為何？ 請實作輸入特徵標準化 (feature normalization)，並比較是否應用此技巧，會對於你的模型有何影響。 应用特征标准化 未应用特征标准化 train 88.569% 83.950% dev 87.597% 83.892% 李弘毅老师ML课程主页]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李弘毅ML课程作业-02：线性二元分类器]]></title>
    <url>%2F2020%2F03%2F21%2FMLHW2%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[作业描述​ 根据人们的个人资料，判断其年收入是否高于$50, 000。这其实是个二元分类问题，本文将以 logistic regression 和 generative model 两种方法达成分类目的。 ​ 作业提供了五个文件，其中在训练过程中，只有X_train、Y_train 和 X_test 这三个经过处理的档案会被使用到，train.csv 和 test.csv 是原始资料，供以参考。 Logistic Regression数据准备​ 先打开看一下数据集长什么样子。训练集数据的各种属性资料都已经数字化了；训练集的标签就是二元化的标签，年收入高于$50, 000为1，否则为0。 ​ 读入数据。由于第一行是数据说明，不必存到数组中，所以用next()函数跳过。数据集的第一列是id号，也不必存到数组中，所以数组存入的数据是从第二行第二列开始直到最后。 123456789101112131415161718import numpy as npnp.random.seed(0)X_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './output_&#123;&#125;.csv'# Parse csv files to numpy arraywith open(X_train_fpath) as f: next(f) X_train = np.array([line.strip('\n').split(',')[1:] for line in f], dtype=float)with open(Y_train_fpath) as f: next(f) Y_train = np.array([line.strip('\n').split(',')[1] for line in f], dtype=float)with open(X_test_fpath) as f: next(f) X_test = np.array([line.strip('\n').split(',')[1:] for line in f], dtype=float) ​ 先定义几个辅助函数。_normalize 函数用来对于数据集的特定列（某属性特征）进行归一化，注释有详细说明。_train_dev_split 函数用于将训练数据集划分为training set和 development set。我们拿不到 testing set 的正确标签，要用development set 来评估性能，而且也能防止过拟合。 12345678910111213141516171819202122232425262728293031def _normalize(X, train=True, specified_column = None, X_mean = None, X_std = None): # This function normalizes specific columns of X. # The mean and standard variance of training data will be reused when processing testing data. # # Arguments: # X: data to be processed # train: 'True' when processing training data, 'False' for testing data # specific_column: indexes of the columns that will be normalized. If 'None', all columns # will be normalized. # X_mean: mean value of training data, used when train = 'False' # X_std: standard deviation of training data, used when train = 'False' # Outputs: # X: normalized data # X_mean: computed mean value of training data # X_std: computed standard deviation of training data if specified_column == None: specified_column = np.arange(X.shape[1]) if train: X_mean = np.mean(X[:, specified_column], 0).reshape(1, -1) X_std = np.std(X[:, specified_column], 0).reshape(1, -1) X[:, specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8) return X, X_mean, X_stddef _train_dev_split(X, Y, dev_ratio=0.25): """This function splits data into training set and development set.""" train_size = int(len(X)*(1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:] ​ 进行归一化和划分训练集。training set 大小：48, 830 × 510；development set 大小：5, 426 × 510；testing set 大小：27, 622 × 510。 123456789101112# Normalize training and testing dataX_train, X_mean, X_std = _normalize(X_train, train=True)X_test, _, _ = _normalize(X_test, train=False, specified_column=None, X_mean=X_mean, X_std=X_std)# Split data into training set and development setdev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio)train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1] 一些辅助函数​ _shuffle 方法将X和Y的所有元素同时随机排序，X中某元素依然对应着在Y中对应着原先所对应的元素（data-label pair 不变）。_f 就是 logistic 回归函数，函数输出值在0到1之间，越接近于1，表示年收入高于$50, 000的概率越大。 \sigma (z) = 1/(1+e^{-z}) f_{w,b}=\sigma （\sum_{i}w_{i}x_{i}+b）12345678910111213141516171819202122232425262728293031def _shuffle(X, Y): # This function shuffles two equal-length list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return (X[randomize], Y[randomize])def _sigmoid(z): # Sigmoid function can be used to calculate probability. # To avoid overflow, minimum/maximum output value is set. return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))def _f(X, w, b): # This is the logistic regression function, parameterized by w and b # # Arguements: # X: input data, shape = [batch_size, data_dimension] # w: weight vector, shape = [data_dimension, ] # b: bias, scalar # Output: # predicted probability of each row of X being positively labeled, shape = [batch_size, ] return _sigmoid(np.matmul(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X # by rounding the result of logistic regression function. return np.round(_f(X, w, b)).astype(np.int) def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc 训练​ 使用的是交叉熵损失函数： L(f)=\sum_{n}C(f(x^{n}),\widehat{y}^{n}) C(f(x^{n}),\widehat{y}^{n})=-[\widehat{y}^{n}lnf(x^{n})+(1-\widehat{y}^{n})ln(1-f(x^{n}))]利用损失函数对权重的梯度值，权重更新公式如下。 w_{i} \leftarrow w_{i}-\eta \sum_{i}[-(\widehat{y}^{n}-f_{w,b}(x^{n}))x_{i}^{n}]交叉熵和梯度值计算代码： 123456789101112131415161718def _cross_entropy_loss(y_pred, Y_label): # This function computes the cross entropy. # # Arguements: # y_pred: probabilistic predictions, float vector # Y_label: ground truth labels, bool vector # Output: # cross entropy, scalar cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred)) return cross_entropydef _gradient(X, Y_label, w, b): # This function computes the gradient of cross entropy loss with respect to weight w and bias b. y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = -np.sum(pred_error * X.T, 1) b_grad = -np.sum(pred_error) return w_grad, b_grad ​ 使用mini-batch gradient descent 来训练。“訓練資料被分為許多小批次，針對每一個小批次，我們分別計算其梯度以及損失，並根據該批次來更新模型的參數。當一次迴圈完成，也就是整個訓練集的所有小批次都被使用過一次以後，我們將所有訓練資料打散並且重新分成新的小批次，進行下一個迴圈，直到事先設定的迴圈數量達成為止。”（来自作业demo） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Zero initialization for weights ans biasw = np.zeros((data_dim,)) b = np.zeros((1,))# Some parameters for training max_iter = 10batch_size = 8learning_rate = 0.2# Keep the loss and accuracy at every iteration for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Calcuate the number of parameter updatesstep = 1# Iterative trainingfor epoch in range(max_iter): # Random shuffle at the begging of each epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for idx in range(int(np.floor(train_size / batch_size))): X = X_train[idx*batch_size:(idx+1)*batch_size] Y = Y_train[idx*batch_size:(idx+1)*batch_size] # Compute the gradient w_grad, b_grad = _gradient(X, Y, w, b) # gradient descent update # learning rate decay with time w = w - learning_rate/np.sqrt(step) * w_grad b = b - learning_rate/np.sqrt(step) * b_grad step = step + 1 # Compute loss and accuracy of training set and development set y_train_pred = _f(X_train, w, b) Y_train_pred = np.round(y_train_pred) train_acc.append(_accuracy(Y_train_pred, Y_train)) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) / train_size) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.round(y_dev_pred) dev_acc.append(_accuracy(Y_dev_pred, Y_dev)) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev) / dev_size)print('Training loss: &#123;&#125;'.format(train_loss[-1]))print('Development loss: &#123;&#125;'.format(dev_loss[-1]))print('Training accuracy: &#123;&#125;'.format(train_acc[-1]))print('Development accuracy: &#123;&#125;'.format(dev_acc[-1])) 绘制 loss 和 accuracy 曲线1234567891011121314151617import matplotlib.pyplot as plt# Loss curveplt.plot(train_loss)plt.plot(dev_loss)plt.title('Loss')plt.legend(['train', 'dev'])plt.savefig('loss.png')plt.show()# Accuracy curveplt.plot(train_acc)plt.plot(dev_acc)plt.title('Accuracy')plt.legend(['train', 'dev'])plt.savefig('acc.png')plt.show() 保存测试集结果123456# Predict testing labelspredictions = _predict(X_test, w, b)with open(output_fpath.format('logistic'), 'w') as f: f.write('id,label\n') for i, label in enumerate(predictions): f.write('&#123;&#125;,&#123;&#125;\n'.format(i, label)) Probabilistic Generative Model数据准备​ 训练集和测试集的读入和归一化处理方法与logistic regression 一模一样。但是generative model 有可解析的最佳解，因此不必使用development set。 计算均值和协方差​ 在generative model 中，需要计算出最有可能产生这个训练集的数据分布的参数，也就是均值和协方差。确定了这两个参数，分布就确定了，就可以计算出测试集中某人属于年收入高于$50, 000那一类的概率。两个类别使用同样的协方差。 123456789101112131415161718# Compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])mean_0 = np.mean(X_train_0, axis = 0)mean_1 = np.mean(X_train_1, axis = 0) # Compute in-class covariancecov_0 = np.zeros((data_dim, data_dim))cov_1 = np.zeros((data_dim, data_dim))for x in X_train_0: cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# Shared covariance is taken as a weighted average of individual in-class covariance.cov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train_0.shape[0] + X_train_1.shape[0]) 计算权重和偏差​ 权重矩阵和偏差向量可以直接计算出来。这里求逆矩阵用到了SVD分解。 \boldsymbol{w}=\Sigma^{-1}(\mu_{1}-\mu_{2}) b=-\frac{1}{2}\mu_{1}^{T}\Sigma^{-1}\mu_{1}+\frac{1}{2}\mu_{2}^{T}\Sigma^{-1}\mu_{2}+ln\frac{N_{1}}{N_{2}}其中$\Sigma$表示协方差，$\mu$表示均值，$N$表示样本数。 1234567891011121314# Compute inverse of covariance matrix.# Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.u, s, v = np.linalg.svd(cov, full_matrices=False)inv = np.matmul(v.T * 1 / s, u.T)# Directly compute weights and biasw = np.dot(inv, mean_0 - mean_1)b = (-0.5) * np.dot(mean_0, np.dot(inv, mean_0)) + 0.5 * np.dot(mean_1, np.dot(inv, mean_1))\ + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0]) # Compute accuracy on training setY_train_pred = 1 - _predict(X_train, w, b)print('Training accuracy: &#123;&#125;'.format(_accuracy(Y_train_pred, Y_train))) 保存测试集结果123456# Predict testing labelspredictions = 1 - _predict(X_test, w, b)with open(output_fpath.format('generative'), 'w') as f: f.write('id,label\n') for i, label in enumerate(predictions): f.write('&#123;&#125;,&#123;&#125;\n'.format(i, label)) Report 請比較實作的 generative model 及 logistic regression 的準確率，何者較佳？請解釋為何有這種情況？ Generative Model Logistic Regression train 87.412% 88.569% 請實作 logistic regression 的正規化 (regularization)，並討論其對於你的模型準確率的影響。接著嘗試對正規項使用不同的權重 (lambda)，並討論其影響。 請說明你實作的 best model，其訓練方式和準確率為何？ 請實作輸入特徵標準化 (feature normalization)，並比較是否應用此技巧，會對於你的模型有何影響。 应用特征标准化 未应用特征标准化 train 88.569% 83.950% dev 87.597% 83.892% 李弘毅老师ML课程主页]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李弘毅ML课程作业-01：PM2.5的预测]]></title>
    <url>%2F2020%2F03%2F18%2FMLHW1%2F</url>
    <content type="text"><![CDATA[作业描述​ 给定了训练集train.csv，要求用前9小时的数据预测出第十个小时的PM2.5的值。这是linear regression的作业。 Feature Selection​ train.csv中每个小时给出了18个特征，其实有些特征与PM2.5关系不大。首先画一下相关矩阵热力图来观察一下。 123456789101112131415161718192021222324import numpy as npimport pandas as pddata=pd.read_csv('train.csv')data.drop(['a','b'],axis=1,inplace=True)column=data['c'].unique()data_new=pd.DataFrame(np.zeros([24*240,18]),columns=column)for i in column: aa=data[data['c']==i] aa.drop(['c'],axis=1,inplace=True) aa=np.array(aa) aa[aa=='NR']='0' aa=aa.astype('float32') aa=aa.reshape(1,5760) aa=aa.T data_new[i]=aalabel=np.array(data_new['PM2.5'][9:],dtype='float32')import matplotlib.pyplot as pltimport seaborn as sns# Draw a heatmap with the numeric values in each cellf, ax = plt.subplots(figsize=(9, 6))sns.heatmap(abs(data_new.corr()), fmt="d", linewidths=.5, ax=ax, cmap='Blues')f.savefig('heatmap.png') ​ 这里不关心是负相关还是正相关，所以对相关矩阵取了绝对值。画出来的结果是这样的： 对于PM2.5来说，相关性比较高的有NO2, NO1, NOx, SO2, THC, PM10等特征。 ​ 也可以把每个特征与PM2.5的关系绘制一下。可以观察到部分特征与PM2.5的值关系不大。 Data Preprocessing​ 这里用了作业demo给出的方法。把前9个小时的全部18个特征都作为预测第十小时PM2.5的特征。也就是说，(9*18+1) 个输入特征（加上bias）对应一个输出。输入进行了归一化。 1234567891011121314151617181920212223242526272829data = pd.read_csv('../train.csv', encoding='big5')data = data.iloc[:, 3:]data[data == 'NR'] = 0raw_data = data.to_numpy()month_data = &#123;&#125;for month in range(12): sample = np.empty([18, 480]) for day in range(20): sample[:, day*24:(day+1)*24] = raw_data[18*(20*month+day):18*(20*month+day+1)] month_data[month] = samplex = np.empty([12*471, 18*9], dtype=float) # training datay = np.empty([12*471, 1], dtype=float) # training setfor month in range(12): for day in range(20): for hour in range(24): if day == 19 and hour &gt; 14: continue x[month*471+day*24+hour, :] = month_data[month][:, day*24+hour:day*24+hour+9].reshape(1,-1) y[month*471+day*24+hour, 0] = month_data[month][9, day*24+hour+9] #value# Normalizationmean_x = np.mean(x, axis=0)std_x = np.std(x, axis=0)for i in range(len(x)): for j in range(len(x[0])): if std_x[j] != 0: x[i][j] = (x[i][j] - mean_x[j]) / std_x[j] ​ 处理完后，张量x就是输入（前9小时的所有特征），y就是输出（第十小时PM2.5值）。 用不同的学习率训练1234567891011121314151617181920212223242526# trainingdim = 18*9 + 1w = np.ones([dim, 1])x = np.concatenate((np.ones([12*471, 1]), x), axis = 1).astype(float)learning_rate = 100iter_time = 1000adagrad = np.zeros([dim, 1])eps = 1e-10training_loss = []for t in range(iter_time): loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))/471/12) # rmse training_loss.append(loss) if t % 100 == 0: print(str(t)+":"+str(loss)) gradient = 2 * np.dot(x.transpose(), np.dot(x, w) - y) # dim*1 adagrad += gradient ** 2 w = w - learning_rate * gradient / np.sqrt(adagrad + eps)np.save('weight.npy', w)# Save loss datafileObject = open('lr_loss_'+str(learning_rate)+'.txt', 'w')for ip in training_loss: fileObject.write(str(ip)) fileObject.write('\n')fileObject.close() 在相同的参数下，分别用0.1，10，100，1000的学习率训练，看一下收敛的效果。学习率为0.1时收敛非常慢，学习率太大（如1000）时又无法收敛到最小值。 李弘毅老师ML课程主页 correlation matrix heatmap reference]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我们的故事]]></title>
    <url>%2F2019%2F09%2F27%2F%E6%88%91%E4%BB%AC%E7%9A%84%E6%95%85%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[1他突然说要来看樱花的时候，我真的以为他是来看花的。 虽然在同一座城市，但是我们的联系实在少的可怜。我时常忘记，在这个离家近一千公里的地方，还有一个同乡的存在。直到他说，4年了，总该来你的学校看一次樱花🌸。当他提出要我当导游的时候，我心里是万般不乐意的。多年的内向性格令我无比抗拒和不熟悉的人往来。但是他说作为报酬，会请我吃饭。于是我应邀了。 看花的那天人很多。他见到我，第一句话是：“我们多久没见了？”是的，自从大一见了两三次之后，我们这几年都没见过。虽然我们的学校只隔了一条街。那天我很拘束，后来慢慢聊开了，问了他一些我后来觉得很蠢的问题。我注意到他和大多数看花人不一样，他并没有对🌸表现出很大的兴趣。只是那时我以为，🌸不够美，不足以吸引他。 2后来我们的来往突然多了起来。我去看他的篮球赛，我们去看电影看夜景。再到后来，他突然牵我的手，我慌慌张张挣脱，才明白过来，他没把我当兄弟。那段时间我经历了生活和学习上的一些挫败，对于他表现出来的心意考虑了很多。犹豫了很多天之后，在他第三次尝试牵手的时候，我没有再挣脱。 只是我没想过，所有我以为的偶然，从来都不是偶然。 高考放榜之后偶然问起他的去向，惊讶发现我们将要去往同一座城市。 某年生日零点收到他的红包，我惊喜于他竟然记得我的生日。 后来他说，“我喜欢你的时间，要比你想象中长很多。” 3故事是他后来告诉我的。 某天我问他，“我很好奇你是什么时候开始对我有想法的？” ”如果是说那种朦胧的感情，我很早就有了。3月份见到你，发现你还是我想象中的样子，所以确定了内心的想法。” 他说，在很早很早的时候，他对我就有很模糊朦胧的感情。大概是初中某年我找他爬山的时候？他说，“我还记得你当时的样子，留着短发，很可爱。” 他说，后来他选择脱离大环境，到另一个地方读高中，却会偶尔想起我。 他说，大二的某个夜晚，他心里很郁闷，夜里出来走走，不觉间走到我的学校。明明想叫我出来，却又害怕突然的打扰，最后只是随便寒暄了几句。 他说，大学怂了四年，终于迈出一步，以看花为借口靠近我，然后约我看电影，然后尝试着牵手，在被拒绝之后心情很低落。但还是鼓起勇气试了许多次。 他还说，去年看到我说要去北京，他就疯狂联系北京的学校，只是因为种种原因未成行。 “从我的学校到你的学校这一步，我走了四年。很庆幸，终于走到了。” 4他听着李宗盛的《鬼迷心窍》，一步一步向我走来。后知后觉的我，庆幸着自己是这个十年故事的女主角。最近听JJ的新歌，感觉有一句歌词很适合总结这篇文章：“这故事开始一个人，我认真写成了我们”。 谢谢你的等待，谢谢你的认真。未来有你，于我幸甚。 ​]]></content>
      <tags>
        <tag>personal stuff</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F09%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
