<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[PyQt的安装以及在PyCharm上的部署]]></title>
    <url>%2F2020%2F05%2F12%2Fsetup_pyqt%E5%9C%A8pycharm%E4%B8%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[PyQt5安装12pip install pyqt5pip install pyqt5-tools 第二个命令包含了designer的安装。我的designer.exe安装路径在E:\Anaconda3\Library\bin 可以用Cotana搜索designer，然后右键打开文件所在位置，直接定位它的安装路径，后面要用到。 designer在PyCharm上部署File-&gt;Settings-&gt;Tools-&gt;External Tools 点击“+”来增加外部工具。 (1) 增加QT设计界面“Qt Designer” — 这个就是设计Qt界面的工具 Program选择PyQt安装目录中 designer.exe 的路径 Work directory 使用变量 $ProjectFileDir$ （点击后面的Insert Macro…） (2) 增加“PyUIC” — 这个主要是用来将 Qt界面 转换成 py代码 Program选择PyQt安装目录中 pyuic5.bat 的路径（我的依然在E:\Anaconda3\Library\bin里） parameters设置为$FileName$ -o $FileNameWithoutExtension$.py Work directory 设置为 $ProjectFileDir$ （点击后面的Insert Macro…） 点击确认就设置好了。返回去后通过Tools可以看到： 设计GUI依照上图，点击PyQt Designer, 在弹出来的界面中选择Wdiget，然后点击创建。 在窗口添加控件，Lable、pushButton、checkBox、lineEdit等： 把.ui文件保存到当前项目目录中，然后右键点击.ui文件： 点击PyUIC即可将.ui转成.py文件。 在login.py中添加： 12345678if __name__=="__main__": import sys app=QtWidgets.QApplication(sys.argv) widget=QtWidgets.QWidget() ui=Ui_form() ui.setupUi(widget) widget.show() sys.exit(app.exec_()) 运行login.py，就可以看到这个页面了。 QtDesigner的安装 设计GUI]]></content>
      <categories>
        <category>Setup</category>
      </categories>
      <tags>
        <tag>setup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows更改pip镜像源及解决总是timeout的情况]]></title>
    <url>%2F2020%2F05%2F12%2Fsetup_pip%E9%95%9C%E5%83%8F%E6%BA%90%2F</url>
    <content type="text"><![CDATA[pip安装事情的起因是我想安装PyQt5： 12pip install pyqt5pip install pyqt5-tools 当然这是最慢的方法，于是可以用镜像源安装： pip安装使用国内镜像源1234567891011清华：https://pypi.tuna.tsinghua.edu.cn/simple阿里云：http://mirrors.aliyun.com/pypi/simple/中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/华中理工大学：http://pypi.hustunique.com/山东理工大学：http://pypi.sdutlinux.org/ 豆瓣：http://pypi.douban.com/simple/ 临时使用的话加参数-i，例如： 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyqt5 然而依然总是timeout，于是： 设置pip配置文件配置文件地址：（如果没有pip.ini文件，就自己新建编辑一个） 1C:\ProgramData\pip\pip.ini 配置文件内容，将召唤timeout的时长设置得长一些 12345678[global]timeout = 60index-url = http://pypi.douban.com/simpletrusted-host = pypi.douban.com[install]use-mirrors = truemirrors = http://pypi.douban.comtrusted-host = pypi.douban.com 设置好之后，再用 1pip install pyqt5 会显示现在使用的是豆瓣镜像源，速度也飞起来了。 pip安装PyQt5 pip使用国内镜像源 pip配置文件设置]]></content>
      <categories>
        <category>Setup</category>
      </categories>
      <tags>
        <tag>setup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch计算模型运算量的工具--torchstat]]></title>
    <url>%2F2020%2F05%2F11%2Fpytorch_torchstat%2F</url>
    <content type="text"><![CDATA[说明与安装这个包可以计算出一个网络模型的参数量和运算量，甚至给出每一层的运算量，比如： 安装方法为： 1pip install torchstat 示例12345678910111213141516171819202122232425262728import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torchstat import statclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(56180, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 56180) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1)if __name__ == '__main__': model = Net() stat(model, (3, 224, 224)) 然后在命令行输入 1torchstat -f example.py -m Net 如果要更改输入图像的尺寸，只改example.py里的（3，224，224）没有起作用，于是我使用了-s选项： 1torchstat -f example.py -m Net -s &#39;3x32x32&#39; 这里选项内容是用字符串表示的，‘x’就是字母x。 另一个示例12345from torchstat import statimport torchvision.models as modelsmodel = models.resnet18()stat(model, (3, 224, 224)) torchstat的Github]]></content>
      <categories>
        <category>PyTorch Tools</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NAS：DARTS论文解读]]></title>
    <url>%2F2020%2F04%2F26%2FNAS_DARTS%2F</url>
    <content type="text"><![CDATA[Search Space搜索cell作为网络结构的构件。cell是包含N个结点的有序序列的有向无环图。结点$x^{(i)}$是隐藏表达（比如卷积网络的特征图），而有向边$(i ,j)$则关联着变换$x^{(i)}$的一些操作$o^{(i,j)}$。文章中假定一个cell中有两个输入节点和一个输出节点。对于卷积cell，输入节点被定义为当前层的前面两层的cell的输出。当前cell的输出是对所有中间节点使用一个压缩操作（比如拼接）得到的。 每一个中间结点基于其所有前向操作计算得到： x^{(j)}=\sum_{i]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李弘毅ML课程作业-03：CNN]]></title>
    <url>%2F2020%2F04%2F08%2FMLHW3%2F</url>
    <content type="text"><![CDATA[数据集准备本次作业任务是食物分类，一共有11种食物。作业提供了三个数据集：training set，validation set，testing set。打开 training set 可以看到数据集是这样的： 数据集由许多张不同种类的食物图片组成，图片的文件名格式是“类别号_图像编号”，类别号为0~10。我们需要将数据集读入程序，并调整为pytorch数据集的格式。 读入数据集 必要的 package 的导入 12345678910# Import需要的套件import osimport numpy as npimport cv2import torchimport torch.nn as nnimport torchvision.transforms as transformsimport pandas as pdfrom torch.utils.data import DataLoader, Datasetimport time 读取照片并存放到 numpy array 中 1234567891011121314def readfile(path, label): # label 是一個 boolean variable，代表需不需要回傳 y 值 image_dir = sorted(os.listdir(path)) x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8) y = np.zeros((len(image_dir)), dtype=np.uint8) for i, file in enumerate(image_dir): img = cv2.imread(os.path.join(path, file)) x[i, :, :] = cv2.resize(img,(128, 128)) if label: y[i] = int(file.split("_")[0]) if label: return x, y else: return x 经过 readfile 函数的处理之后，x是储存了图像数据的 numpy 数组，y是储存了图像标签（类别号）的 numpy 数组。其中，y[i] 就是图像 x[i] 的标签。此外，数据集中的图像尺寸不一，readfile 函数在读入时全部 resize 成了 128×128 大小。 使用 readfile 函数将三个数据集读取进来 12345678workspace_dir = './food-11'print("Reading data")train_x, train_y = readfile(os.path.join(workspace_dir, "training"), True)print("Size of training data = &#123;&#125;".format(len(train_x)))val_x, val_y = readfile(os.path.join(workspace_dir, "validation"), True)print("Size of validation data = &#123;&#125;".format(len(val_x)))test_x = readfile(os.path.join(workspace_dir, "testing"), False)print("Size of Testing data = &#123;&#125;".format(len(test_x))) 调整为 PyTorch 数据集格式在 Pytorch 中，我们可以利用 torch.utils.data 的 Dataset 及 DataLoader 來”包装” data，使后续的 training 及 testing 更为方便。 Dataset 需要 overload 两个函数：__len__ 及 __getitem__ __len__ 必须要回传 dataset 的大小，而 __getitem__ 则定义了当程序利用 [ ] 取值時，dataset 应该要怎么回传资料。 12345678910111213141516171819202122232425262728293031#training 時做 data augmentationtrain_transform = transforms.Compose([ transforms.ToPILImage(), transforms.RandomHorizontalFlip(), #隨機將圖片水平翻轉 transforms.RandomRotation(15), #隨機旋轉圖片 transforms.ToTensor(), #將圖片轉成 Tensor，並把數值normalize到[0,1](data normalization)])#testing 時不需做 data augmentationtest_transform = transforms.Compose([ transforms.ToPILImage(), transforms.ToTensor(),])class ImgDataset(Dataset): def __init__(self, x, y=None, transform=None): self.x = x # label is required to be a LongTensor self.y = y if y is not None: self.y = torch.LongTensor(y) self.transform = transform def __len__(self): return len(self.x) def __getitem__(self, index): X = self.x[index] if self.transform is not None: X = self.transform(X) if self.y is not None: Y = self.y[index] return X, Y else: return X 12345batch_size = 128train_set = ImgDataset(train_x, train_y, train_transform)val_set = ImgDataset(val_x, val_y, test_transform)train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False) CNN模型1234567891011121314151617181920212223242526272829303132333435363738394041424344class Classifier(nn.Module): def __init__(self): super(Classifier, self).__init__() #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding) #torch.nn.MaxPool2d(kernel_size, stride, padding) #input 維度 [3, 128, 128] self.cnn = nn.Sequential( nn.Conv2d(3, 64, 3, 1, 1), # [64, 128, 128] nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [64, 64, 64] nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64] nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [128, 32, 32] nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32] nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [256, 16, 16] nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16] nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [512, 8, 8] nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8] nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [512, 4, 4] ) self.fc = nn.Sequential( nn.Linear(512*4*4, 1024), nn.ReLU(), nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, 11) ) def forward(self, x): out = self.cnn(x) out = out.view(out.size()[0], -1) return self.fc(out) 训练使用 training set 训练，并利用 validation set 寻找好的参数 123456789101112131415161718192021222324252627282930313233343536model = Classifier().cuda()loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLossoptimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer 使用 Adamnum_epoch = 30for epoch in range(num_epoch): epoch_start_time = time.time() train_acc = 0.0 train_loss = 0.0 val_acc = 0.0 val_loss = 0.0 model.train() # 確保 model 是在 train model (開啟 Dropout 等...) for i, data in enumerate(train_loader): optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零 train_pred = model(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數 batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上） batch_loss.backward() # 利用 back propagation 算出每個參數的 gradient optimizer.step() # 以 optimizer 用 gradient 更新參數值 train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy()) train_loss += batch_loss.item() model.eval() with torch.no_grad(): for i, data in enumerate(val_loader): val_pred = model(data[0].cuda()) batch_loss = loss(val_pred, data[1].cuda()) val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy()) val_loss += batch_loss.item() #將結果 print 出來 print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \ (epoch + 1, num_epoch, time.time()-epoch_start_time, \ train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__())) 得到好的参数后，我们使用 training set 和 validation set 共同训练（资料量变多，模型效果较好） 1234train_val_x = np.concatenate((train_x, val_x), axis=0)train_val_y = np.concatenate((train_y, val_y), axis=0)train_val_set = ImgDataset(train_val_x, train_val_y, train_transform)train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True) 12345678910111213141516171819202122232425model_best = Classifier().cuda()loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLossoptimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adamnum_epoch = 30for epoch in range(num_epoch): epoch_start_time = time.time() train_acc = 0.0 train_loss = 0.0 model_best.train() for i, data in enumerate(train_val_loader): optimizer.zero_grad() train_pred = model_best(data[0].cuda()) batch_loss = loss(train_pred, data[1].cuda()) batch_loss.backward() optimizer.step() train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy()) train_loss += batch_loss.item() #將結果 print 出來 print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \ (epoch + 1, num_epoch, time.time()-epoch_start_time, \ train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__())) 测试利用刚刚 train 好的 model 进行 prediction 1234567891011121314151617test_set = ImgDataset(test_x, transform=test_transform)test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)model_best.eval()prediction = []with torch.no_grad(): for i, data in enumerate(test_loader): test_pred = model_best(data.cuda()) test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1) for y in test_label: prediction.append(y) #將結果寫入 csv 檔with open("predict.csv", 'w') as f: f.write('Id,Category\n') for i, y in enumerate(prediction): f.write('&#123;&#125;,&#123;&#125;\n'.format(i, y))]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[罗翔讲刑法--金句笔记]]></title>
    <url>%2F2020%2F03%2F21%2F%E7%BD%97%E7%BF%94%E9%87%91%E5%8F%A5%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[我们之所以做一个道德的人，不是为了追求幸福，而只是因为这本身就无愧于我们已经拥有的幸福。 一个人不能处分自己最重要的利益，因为自由不能以彻底放弃自由为代价。（这样的自由一定会导致强者对弱者的剥削。） 法益作为入罪的基础，而伦理作为出罪的依据。 犯罪是可怕的，但比犯罪更可怕的是不受约束的刑罚权力。 一次不公正的审判，其恶果甚至超过十次犯罪。因为犯罪虽是无视法律——好比污染了水流，而不公正的审判则毁坏法律——好比污染水源。（培根） 自从有刑法存在国家代替受害人私刑报复时开始，国家就承担了双重责任，因此表现出它的悖论性。刑法不仅要面对犯罪人保护国家，也要面对国家保护犯罪人，不单面对犯罪人也要面对检察官保护市民，成为公民反对私法专横和错误的大宪章。（德国法学家拉德布鲁赫） 如果刑罚权不受限制，那么一切正义都有可能被架空，而且往往是以正义的名义来架空正义。 权力导致腐败，绝对权力导致绝对腐败。 刑法是一根带哨子的皮鞭。 当立法权和司法权合二为一，独裁就不可避免，自由就荡然无存。（孟德斯鸠） 法治的精神在于限权。 如果衣服上出现了褶皱，司法机关可以用熨斗把它熨平，但如果衣服上出现了一个大洞，那就必须取决于立法机关把它织补。（英国丹宁勋爵） 法律是一种平衡的艺术，要在诸多对立价值中寻找一个平衡点。 有一种鸟是永远也关不住的，因为它的每片羽翼上都沾满了自由的光辉。（《肖申克的救赎》经典台词） 法律要倾听民众的声音，但是要超越民众的偏见。]]></content>
      <categories>
        <category>Law</category>
      </categories>
      <tags>
        <tag>law</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李弘毅ML课程作业-02：线性二元分类器]]></title>
    <url>%2F2020%2F03%2F21%2FMLHW2%2F</url>
    <content type="text"><![CDATA[作业描述​ 根据人们的个人资料，判断其年收入是否高于$50, 000。这其实是个二元分类问题，本文将以 logistic regression 和 generative model 两种方法达成分类目的。 ​ 作业提供了五个文件，其中在训练过程中，只有X_train、Y_train 和 X_test 这三个经过处理的档案会被使用到，train.csv 和 test.csv 是原始资料，供以参考。 Logistic Regression数据准备​ 先打开看一下数据集长什么样子。训练集数据的各种属性资料都已经数字化了；训练集的标签就是二元化的标签，年收入高于$50, 000为1，否则为0。 ​ 读入数据。由于第一行是数据说明，不必存到数组中，所以用next()函数跳过。数据集的第一列是id号，也不必存到数组中，所以数组存入的数据是从第二行第二列开始直到最后。 123456789101112131415161718import numpy as npnp.random.seed(0)X_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './output_&#123;&#125;.csv'# Parse csv files to numpy arraywith open(X_train_fpath) as f: next(f) X_train = np.array([line.strip('\n').split(',')[1:] for line in f], dtype=float)with open(Y_train_fpath) as f: next(f) Y_train = np.array([line.strip('\n').split(',')[1] for line in f], dtype=float)with open(X_test_fpath) as f: next(f) X_test = np.array([line.strip('\n').split(',')[1:] for line in f], dtype=float) ​ 先定义几个辅助函数。_normalize 函数用来对于数据集的特定列（某属性特征）进行归一化，注释有详细说明。_train_dev_split 函数用于将训练数据集划分为training set和 development set。我们拿不到 testing set 的正确标签，要用development set 来评估性能，而且也能防止过拟合。 12345678910111213141516171819202122232425262728293031def _normalize(X, train=True, specified_column = None, X_mean = None, X_std = None): # This function normalizes specific columns of X. # The mean and standard variance of training data will be reused when processing testing data. # # Arguments: # X: data to be processed # train: 'True' when processing training data, 'False' for testing data # specific_column: indexes of the columns that will be normalized. If 'None', all columns # will be normalized. # X_mean: mean value of training data, used when train = 'False' # X_std: standard deviation of training data, used when train = 'False' # Outputs: # X: normalized data # X_mean: computed mean value of training data # X_std: computed standard deviation of training data if specified_column == None: specified_column = np.arange(X.shape[1]) if train: X_mean = np.mean(X[:, specified_column], 0).reshape(1, -1) X_std = np.std(X[:, specified_column], 0).reshape(1, -1) X[:, specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8) return X, X_mean, X_stddef _train_dev_split(X, Y, dev_ratio=0.25): """This function splits data into training set and development set.""" train_size = int(len(X)*(1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:] ​ 进行归一化和划分训练集。training set 大小：48, 830 × 510；development set 大小：5, 426 × 510；testing set 大小：27, 622 × 510。 123456789101112# Normalize training and testing dataX_train, X_mean, X_std = _normalize(X_train, train=True)X_test, _, _ = _normalize(X_test, train=False, specified_column=None, X_mean=X_mean, X_std=X_std)# Split data into training set and development setdev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio)train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1] 一些辅助函数​ _shuffle 方法将X和Y的所有元素同时随机排序，X中某元素依然对应着在Y中对应着原先所对应的元素（data-label pair 不变）。_f 就是 logistic 回归函数，函数输出值在0到1之间，越接近于1，表示年收入高于$50, 000的概率越大。 \sigma (z) = 1/(1+e^{-z}) f_{w,b}=\sigma （\sum_{i}w_{i}x_{i}+b）12345678910111213141516171819202122232425262728293031def _shuffle(X, Y): # This function shuffles two equal-length list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return (X[randomize], Y[randomize])def _sigmoid(z): # Sigmoid function can be used to calculate probability. # To avoid overflow, minimum/maximum output value is set. return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))def _f(X, w, b): # This is the logistic regression function, parameterized by w and b # # Arguements: # X: input data, shape = [batch_size, data_dimension] # w: weight vector, shape = [data_dimension, ] # b: bias, scalar # Output: # predicted probability of each row of X being positively labeled, shape = [batch_size, ] return _sigmoid(np.matmul(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X # by rounding the result of logistic regression function. return np.round(_f(X, w, b)).astype(np.int) def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc 训练​ 使用的是交叉熵损失函数： L(f)=\sum_{n}C(f(x^{n}),\widehat{y}^{n}) C(f(x^{n}),\widehat{y}^{n})=-[\widehat{y}^{n}lnf(x^{n})+(1-\widehat{y}^{n})ln(1-f(x^{n}))]利用损失函数对权重的梯度值，权重更新公式如下。 w_{i} \leftarrow w_{i}-\eta \sum_{i}[-(\widehat{y}^{n}-f_{w,b}(x^{n}))x_{i}^{n}]交叉熵和梯度值计算代码： 123456789101112131415161718def _cross_entropy_loss(y_pred, Y_label): # This function computes the cross entropy. # # Arguements: # y_pred: probabilistic predictions, float vector # Y_label: ground truth labels, bool vector # Output: # cross entropy, scalar cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred)) return cross_entropydef _gradient(X, Y_label, w, b): # This function computes the gradient of cross entropy loss with respect to weight w and bias b. y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = -np.sum(pred_error * X.T, 1) b_grad = -np.sum(pred_error) return w_grad, b_grad ​ 使用mini-batch gradient descent 来训练。“訓練資料被分為許多小批次，針對每一個小批次，我們分別計算其梯度以及損失，並根據該批次來更新模型的參數。當一次迴圈完成，也就是整個訓練集的所有小批次都被使用過一次以後，我們將所有訓練資料打散並且重新分成新的小批次，進行下一個迴圈，直到事先設定的迴圈數量達成為止。”（来自作业demo） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Zero initialization for weights ans biasw = np.zeros((data_dim,)) b = np.zeros((1,))# Some parameters for training max_iter = 10batch_size = 8learning_rate = 0.2# Keep the loss and accuracy at every iteration for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Calcuate the number of parameter updatesstep = 1# Iterative trainingfor epoch in range(max_iter): # Random shuffle at the begging of each epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for idx in range(int(np.floor(train_size / batch_size))): X = X_train[idx*batch_size:(idx+1)*batch_size] Y = Y_train[idx*batch_size:(idx+1)*batch_size] # Compute the gradient w_grad, b_grad = _gradient(X, Y, w, b) # gradient descent update # learning rate decay with time w = w - learning_rate/np.sqrt(step) * w_grad b = b - learning_rate/np.sqrt(step) * b_grad step = step + 1 # Compute loss and accuracy of training set and development set y_train_pred = _f(X_train, w, b) Y_train_pred = np.round(y_train_pred) train_acc.append(_accuracy(Y_train_pred, Y_train)) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) / train_size) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.round(y_dev_pred) dev_acc.append(_accuracy(Y_dev_pred, Y_dev)) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev) / dev_size)print('Training loss: &#123;&#125;'.format(train_loss[-1]))print('Development loss: &#123;&#125;'.format(dev_loss[-1]))print('Training accuracy: &#123;&#125;'.format(train_acc[-1]))print('Development accuracy: &#123;&#125;'.format(dev_acc[-1])) 绘制 loss 和 accuracy 曲线1234567891011121314151617import matplotlib.pyplot as plt# Loss curveplt.plot(train_loss)plt.plot(dev_loss)plt.title('Loss')plt.legend(['train', 'dev'])plt.savefig('loss.png')plt.show()# Accuracy curveplt.plot(train_acc)plt.plot(dev_acc)plt.title('Accuracy')plt.legend(['train', 'dev'])plt.savefig('acc.png')plt.show() 保存测试集结果123456# Predict testing labelspredictions = _predict(X_test, w, b)with open(output_fpath.format('logistic'), 'w') as f: f.write('id,label\n') for i, label in enumerate(predictions): f.write('&#123;&#125;,&#123;&#125;\n'.format(i, label)) Probabilistic Generative Model数据准备​ 训练集和测试集的读入和归一化处理方法与logistic regression 一模一样。但是generative model 有可解析的最佳解，因此不必使用development set。 计算均值和协方差​ 在generative model 中，需要计算出最有可能产生这个训练集的数据分布的参数，也就是均值和协方差。确定了这两个参数，分布就确定了，就可以计算出测试集中某人属于年收入高于$50, 000那一类的概率。两个类别使用同样的协方差。 123456789101112131415161718# Compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])mean_0 = np.mean(X_train_0, axis = 0)mean_1 = np.mean(X_train_1, axis = 0) # Compute in-class covariancecov_0 = np.zeros((data_dim, data_dim))cov_1 = np.zeros((data_dim, data_dim))for x in X_train_0: cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# Shared covariance is taken as a weighted average of individual in-class covariance.cov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train_0.shape[0] + X_train_1.shape[0]) 计算权重和偏差​ 权重矩阵和偏差向量可以直接计算出来。这里求逆矩阵用到了SVD分解。 \boldsymbol{w}=\Sigma^{-1}(\mu_{1}-\mu_{2}) b=-\frac{1}{2}\mu_{1}^{T}\Sigma^{-1}\mu_{1}+\frac{1}{2}\mu_{2}^{T}\Sigma^{-1}\mu_{2}+ln\frac{N_{1}}{N_{2}}其中$\Sigma$表示协方差，$\mu$表示均值，$N$表示样本数。 1234567891011121314# Compute inverse of covariance matrix.# Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.u, s, v = np.linalg.svd(cov, full_matrices=False)inv = np.matmul(v.T * 1 / s, u.T)# Directly compute weights and biasw = np.dot(inv, mean_0 - mean_1)b = (-0.5) * np.dot(mean_0, np.dot(inv, mean_0)) + 0.5 * np.dot(mean_1, np.dot(inv, mean_1))\ + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0]) # Compute accuracy on training setY_train_pred = 1 - _predict(X_train, w, b)print('Training accuracy: &#123;&#125;'.format(_accuracy(Y_train_pred, Y_train))) 保存测试集结果123456# Predict testing labelspredictions = 1 - _predict(X_test, w, b)with open(output_fpath.format('generative'), 'w') as f: f.write('id,label\n') for i, label in enumerate(predictions): f.write('&#123;&#125;,&#123;&#125;\n'.format(i, label)) Report 請比較實作的 generative model 及 logistic regression 的準確率，何者較佳？請解釋為何有這種情況？ Generative Model Logistic Regression train 87.412% 88.569% 請實作 logistic regression 的正規化 (regularization)，並討論其對於你的模型準確率的影響。接著嘗試對正規項使用不同的權重 (lambda)，並討論其影響。 請說明你實作的 best model，其訓練方式和準確率為何？ 請實作輸入特徵標準化 (feature normalization)，並比較是否應用此技巧，會對於你的模型有何影響。 应用特征标准化 未应用特征标准化 train 88.569% 83.950% dev 87.597% 83.892% 李弘毅老师ML课程主页]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李弘毅ML课程作业-01：PM2.5的预测]]></title>
    <url>%2F2020%2F03%2F18%2FMLHW1%2F</url>
    <content type="text"><![CDATA[作业描述​ 给定了训练集train.csv，要求用前9小时的数据预测出第十个小时的PM2.5的值。这是linear regression的作业。 Feature Selection​ train.csv中每个小时给出了18个特征，其实有些特征与PM2.5关系不大。首先画一下相关矩阵热力图来观察一下。 123456789101112131415161718192021222324import numpy as npimport pandas as pddata=pd.read_csv('train.csv')data.drop(['a','b'],axis=1,inplace=True)column=data['c'].unique()data_new=pd.DataFrame(np.zeros([24*240,18]),columns=column)for i in column: aa=data[data['c']==i] aa.drop(['c'],axis=1,inplace=True) aa=np.array(aa) aa[aa=='NR']='0' aa=aa.astype('float32') aa=aa.reshape(1,5760) aa=aa.T data_new[i]=aalabel=np.array(data_new['PM2.5'][9:],dtype='float32')import matplotlib.pyplot as pltimport seaborn as sns# Draw a heatmap with the numeric values in each cellf, ax = plt.subplots(figsize=(9, 6))sns.heatmap(abs(data_new.corr()), fmt="d", linewidths=.5, ax=ax, cmap='Blues')f.savefig('heatmap.png') ​ 这里不关心是负相关还是正相关，所以对相关矩阵取了绝对值。画出来的结果是这样的： 对于PM2.5来说，相关性比较高的有NO2, NO1, NOx, SO2, THC, PM10等特征。 ​ 也可以把每个特征与PM2.5的关系绘制一下。可以观察到部分特征与PM2.5的值关系不大。 Data Preprocessing​ 这里用了作业demo给出的方法。把前9个小时的全部18个特征都作为预测第十小时PM2.5的特征。也就是说，(9*18+1) 个输入特征（加上bias）对应一个输出。输入进行了归一化。 1234567891011121314151617181920212223242526272829data = pd.read_csv('../train.csv', encoding='big5')data = data.iloc[:, 3:]data[data == 'NR'] = 0raw_data = data.to_numpy()month_data = &#123;&#125;for month in range(12): sample = np.empty([18, 480]) for day in range(20): sample[:, day*24:(day+1)*24] = raw_data[18*(20*month+day):18*(20*month+day+1)] month_data[month] = samplex = np.empty([12*471, 18*9], dtype=float) # training datay = np.empty([12*471, 1], dtype=float) # training setfor month in range(12): for day in range(20): for hour in range(24): if day == 19 and hour &gt; 14: continue x[month*471+day*24+hour, :] = month_data[month][:, day*24+hour:day*24+hour+9].reshape(1,-1) y[month*471+day*24+hour, 0] = month_data[month][9, day*24+hour+9] #value# Normalizationmean_x = np.mean(x, axis=0)std_x = np.std(x, axis=0)for i in range(len(x)): for j in range(len(x[0])): if std_x[j] != 0: x[i][j] = (x[i][j] - mean_x[j]) / std_x[j] ​ 处理完后，张量x就是输入（前9小时的所有特征），y就是输出（第十小时PM2.5值）。 用不同的学习率训练1234567891011121314151617181920212223242526# trainingdim = 18*9 + 1w = np.ones([dim, 1])x = np.concatenate((np.ones([12*471, 1]), x), axis = 1).astype(float)learning_rate = 100iter_time = 1000adagrad = np.zeros([dim, 1])eps = 1e-10training_loss = []for t in range(iter_time): loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))/471/12) # rmse training_loss.append(loss) if t % 100 == 0: print(str(t)+":"+str(loss)) gradient = 2 * np.dot(x.transpose(), np.dot(x, w) - y) # dim*1 adagrad += gradient ** 2 w = w - learning_rate * gradient / np.sqrt(adagrad + eps)np.save('weight.npy', w)# Save loss datafileObject = open('lr_loss_'+str(learning_rate)+'.txt', 'w')for ip in training_loss: fileObject.write(str(ip)) fileObject.write('\n')fileObject.close() 在相同的参数下，分别用0.1，10，100，1000的学习率训练，看一下收敛的效果。学习率为0.1时收敛非常慢，学习率太大（如1000）时又无法收敛到最小值。 李弘毅老师ML课程主页 correlation matrix heatmap reference]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F09%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
